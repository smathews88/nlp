{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16Izo44JypC33w3A2RzPyYTHdq02uuw5v",
      "authorship_tag": "ABX9TyP+NNVQsNQ7zmrRo/pnV0mM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smathews88/nlp/blob/main/02_ml_NLP_DataPreperationforLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preperation of Input data for LLM**"
      ],
      "metadata": {
        "id": "WUsneYvHxYyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Data**\n",
        "\n",
        "Data preparation and sampling to get input data \"ready\" for the LLM"
      ],
      "metadata": {
        "id": "HbEPi7fn1q6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"business_11.txt\"):\n",
        "    url = (\"https://storage.googleapis.com/kagglesdsdata/datasets/701505/1226200/business/business_11.txt?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240618%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240618T074023Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2a50233309ae50fdbceb07316abeca396bb3cba5cad1be5ef8517835b5a816738390e6ffe16f1bbc74b15811346339ea74135f03d565abdce9435c915ec7d10724d38432ed3135cb95744a088aa7557ae2f636f85e95ac8c3419fb63ea3b75222587206f0625b7f54c85ca3141597f4e641dd69348c6a8944849fe256a2248379c60f797100555cefdd3cf0fe735a5fed0d54df4638b4af0b7901089a685d3ef9c707d2fd9c406ccce1df5596402b1b0173b72201f7a6caf298cc001e7b26aa2d5adf4163ebadbf6935077e76c1c76720fc6936a4c295e0ad0ec59b842549e451ab6e79450f7c07bf05923a640f51759ffd77adfca2df10ac3ab8b03d229808b\")\n",
        "    file_path = \"/content/drive/MyDrive/Colab Notebooks/dataset/business_11.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "zWp6arV023hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"business_11.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHAiJR8i3ALl",
        "outputId": "33319e10-a658-4757-ac26-9f6cde833627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 3494\n",
            "Saab to build Cadillacs in Sweden\n",
            "\n",
            "General Motors, the world's largest car maker, has confirmed tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenizing data and converting tokens to IDs**\n",
        "\n",
        "Tokenizing means breaking text into smaller units, such as individual words and punctuation characters. Each unique token is added to vocabulary in alphabetic order.\n"
      ],
      "metadata": {
        "id": "A0CRt5q59_ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "mDTihpnkfCLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Below is a simple class that perform tokenization and convert tokens to IDs and vice versa.\n",
        "\n",
        "<li> The encode function turns text into token IDs </li>\n",
        "<li> The decode function turns token IDs back into text </li>"
      ],
      "metadata": {
        "id": "8lNuFXOfVDp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "7XWXeomT99zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Bringing Cadillac production to Sweden should help introduce desperately-needed scale to the Saab factory, which currently produces fewer than 130,000 cars per year.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "GGc1YQAf-UJf",
        "outputId": "bdfd33b2-2a96-4624-e4ec-ffff63712b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26, 28, 228, 274, 55, 252, 163, 171, 120, 244, 274, 271, 53, 139, 6, 294, 115, 227, 143, 269, 11, 6, 9, 99, 219, 301, 8]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bringing Cadillac production to Sweden should help introduce desperately-needed scale to the Saab factory, which currently produces fewer than 130, 000 cars per year.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gt5q82IFAa3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Addding Special Context Tokens**\n",
        "It's useful to add some \"special\" tokens for unknown words and to denote the end of a text.**bold text**\n",
        "Some of these special tokens are\n",
        "\n",
        "<li> [BOS] (beginning of sequence) marks the beginning of text </li>\n",
        "<li> [EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on) </li>\n",
        "<li> [PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length) </li>\n",
        "<li> [UNK] to represent works that are not included in the vocabulary </li>"
      ],
      "metadata": {
        "id": "K-fFaDJZ_cfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "E7NdCgt-H6lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "PwCc40_2_bSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)\n",
        "\n",
        "tokenizer.encode(text)\n",
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hVU_x1V9Hjdd",
        "outputId": "2dd5e35f-e24b-46d4-9401-949624a83103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do <|unk|> <|unk|> <|unk|> <|unk|> <|endoftext|> In the <|unk|> <|unk|> of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bytepair Encoding**\n",
        "\n",
        "GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
        "it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
        "For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges"
      ],
      "metadata": {
        "id": "ZiT100hVSO6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC4oDkNoIqfK",
        "outputId": "0d1e6ba9-af87-49c1-b7c8-4cfc2387f38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "rZTARQMygjA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "qcDNlAD6jcCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=8, stride=1, shuffle=False)"
      ],
      "metadata": {
        "id": "UbeZPaBPtZn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "for i in range(inputs.shape[0]):\n",
        "  print(tokenizer.decode(inputs.tolist()[i]),\" ------> \", tokenizer.decode(targets.tolist()[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPG4ioLX3S_3",
        "outputId": "3c5363a8-c6ad-4848-fc5d-4f452b041070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saab to build Cadillacs in  ------>  ab to build Cadillacs in Sweden\n",
            "ab to build Cadillacs in Sweden  ------>   to build Cadillacs in Sweden\n",
            "\n",
            " to build Cadillacs in Sweden\n",
            "  ------>   build Cadillacs in Sweden\n",
            "\n",
            "\n",
            " build Cadillacs in Sweden\n",
            "\n",
            "  ------>   Cadillacs in Sweden\n",
            "\n",
            "General\n",
            " Cadillacs in Sweden\n",
            "\n",
            "General  ------>  illacs in Sweden\n",
            "\n",
            "General Motors\n",
            "illacs in Sweden\n",
            "\n",
            "General Motors  ------>  acs in Sweden\n",
            "\n",
            "General Motors,\n",
            "acs in Sweden\n",
            "\n",
            "General Motors,  ------>   in Sweden\n",
            "\n",
            "General Motors, the\n",
            " in Sweden\n",
            "\n",
            "General Motors, the  ------>   Sweden\n",
            "\n",
            "General Motors, the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating token embeddings**\n",
        "The data is already almost ready for an LLM\n",
        "But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
        "Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
      ],
      "metadata": {
        "id": "Gcv-oOp8cp1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3XdxjR-cdYy",
        "outputId": "f33f356e-838f-43c0-c22a-2901686b2e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 8, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Positional Embedding**\n",
        "<li> Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence </li>\n",
        "<li>Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model. </li>\n",
        "<li>The BytePair encoder has a vocabulary size of 50,257</li>\n"
      ],
      "metadata": {
        "id": "nDckW4koPvFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 8\n",
        "max_length = context_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zehmkYMEPyTE",
        "outputId": "b22e30c3-85d6-4147-e5c1-9cfaadfe163b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5294,  0.5202,  0.1852,  ..., -1.2792,  0.9501,  0.6760],\n",
            "        [-2.1529,  0.1797, -0.6630,  ..., -0.0313,  0.6326,  0.4746],\n",
            "        [ 0.8169,  0.6653, -0.2359,  ...,  0.9724,  1.1933,  0.4391],\n",
            "        ...,\n",
            "        [ 1.4240, -0.2753, -0.2177,  ..., -1.3997,  1.0861, -0.0760],\n",
            "        [ 0.3672, -0.4940,  0.3362,  ..., -1.3847,  0.0193, -0.2987],\n",
            "        [ 1.9855,  0.9215,  0.0335,  ..., -1.1825,  0.2851,  1.3173]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Input Embedding**"
      ],
      "metadata": {
        "id": "H1qjlTsZQwkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYu4kdcBQ2EC",
        "outputId": "0d68dbd4-1bee-4b93-cd02-f9d9d3f17c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 8, 256])\n"
          ]
        }
      ]
    }
  ]
}