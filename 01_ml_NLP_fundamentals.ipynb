{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smathews88/nlp/blob/main/01_ml_NLP_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii1gv1sMNGmR"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFdUUqLp-Cfo"
      },
      "source": [
        "# **1.Steps in NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpiIXCmhPWl"
      },
      "source": [
        "![](https://drive.google.com/uc?id=14VG8EG6LWudifRF4Q9nlRE70WJC2ZRsv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yptd6w22CaNb"
      },
      "source": [
        "# **1.1 Text Cleaning :**\n",
        "\n",
        "Sometimes our acquired data may contain HTML tags, spelling mistakes, or special characters. some techniques to clean our text data.\n",
        "\n",
        "**Unicode Normalization:** if text data may contain symbols, emojis, graphic characters, or special characters. Either we can remove these characters or we can convert this to machine-readable text.  \n",
        "\n",
        "**Regex or Regular Expression:** Regular Expression is the tool that is used for searching the string of specific patterns.  Suppose our data contain phone number, email-Id, and URL. we can find such text using the regular expression. we can keep or remove such text patterns as per requirements.\n",
        "\n",
        "**Spelling corrections:**  When our data is extracted from social media. Spelling mistakes are very common in that case. To overcome this problem we can create a corpus or dictionary of the most common mistype words and replace these common mistakes with the correct word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrZCupGdC3aP"
      },
      "outputs": [],
      "source": [
        " # Unicode Nomalization\n",
        "text = \"GeeksForGeeks ????\"\n",
        "print(text.encode('utf-8'))\n",
        "\n",
        "text1 = 'गीक्स फॉर गीक्स ????'\n",
        "print(text1.encode('utf-8'))\n",
        "Output :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIYUu3Vrm2gu"
      },
      "source": [
        "# **1.2 Pre-Processing**\n",
        "\n",
        "NLP software mainly works at the sentence level and it also expects words to be separated at the minimum level. So, first, we need to Tokenize our text data.\n",
        "\n",
        "# **Tokenization:**\n",
        "Tokenization is the process of segmenting the text into a list of tokens. In the case of sentence tokenization, the token will be sentenced and in the case of word tokenization, it will be the word. It is a good idea to first complete sentence tokenization and then word tokenization, here output will be the list of lists. Tokenization is performed in each & every NLP pipeline.\n",
        "\n",
        "# **Lowercasing**:\n",
        "This step is used to convert all the text to lowercase letters.\n",
        "\n",
        "# **Stop word removal:**\n",
        "Stop words are commonly occurring words in a language such as “the”, “and”, “a”, etc.\n",
        "\n",
        "#**Stemming or lemmatization:**\n",
        "Stemming and lemmatization are used to reduce words to their base form, which can help reduce the vocabulary size and simplify the text. Stemming involves stripping the suffixes from words to get their stem, whereas lemmatization involves reducing words to their base form based on their part of speech.\n",
        "\n",
        "# **Removing digit/punctuation:**\n",
        "This step is used to remove digits and punctuation from the text.\n",
        "\n",
        "# **POS tagging:**\n",
        "POS tagging involves assigning a part of speech tag to each word in a text.\n",
        "\n",
        "#**Entity Recognition (NER):**\n",
        "NER involves identifying and classifying named entities in text, such as people, organizations, and locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "IAXrX9VAqq0S",
        "outputId": "5af56e05-bf1d-4480-a188-e8d9d1bf2059"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,280.0,120.0\" width=\"280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"31.4286%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">important</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"17.1429%\" x=\"31.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">code</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"28.5714%\" x=\"48.5714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">learning</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.8571%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"22.8571%\" x=\"77.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">python</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.5714%\" y1=\"19.2px\" y2=\"48px\" /></svg>",
            "text/plain": [
              "Tree('S', [('important', 'JJ'), ('code', 'NN'), ('learning', 'NN'), ('python', 'NN')])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "import string\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "#nltk.download('words')\n",
        "#!pip install svgling\n",
        "\n",
        "# sample text to be preprocessed\n",
        "text = 'It is very important to code while learning python'\n",
        "def preprocess(text) :\n",
        "  # tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "  # perform stemming and lemmatization\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "  # remove digits and punctuation\n",
        "  cleaned_tokens = [token for token in lemmatized_tokens\n",
        "                  if not token.isdigit() and not token in string.punctuation]\n",
        "\n",
        "  # convert all tokens to lowercase\n",
        "  lowercase_tokens = [token.lower() for token in cleaned_tokens]\n",
        "\n",
        "  # perform part-of-speech (POS) tagging\n",
        "  pos_tags = pos_tag(lowercase_tokens)\n",
        "\n",
        "  # perform named entity recognition (NER)\n",
        "  named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "  return named_entities\n",
        "\n",
        "preprocess(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUy9Mr-Et8MQ"
      },
      "source": [
        "# **1.3  Feature Engineering:**\n",
        "FE is to represent the text in the numeric vector in such a way that the ML algorithm can understand the text attribute. In NLP this process of feature engineering is known as Text Representation or Text Vectorization.\n",
        "\n",
        "There are two most common approaches for Text Representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLM4OrSs9liJ"
      },
      "source": [
        "\n",
        "# **1.3.1 Classical approach**\n",
        "\n",
        "we create a vocabulary of unique words assign a unique id (integer value) for each word. and then replace each word of a sentence with its unique id.  Here each word of vocabulary is treated as a feature\n",
        "\n",
        "\n",
        "**One Hot Encoder:**One Hot Encoding represents each token as a binary vector. First mapped each token to integer values. and then each integer value is represented as a binary vector where all values are 0 except the index of the integer. index of the integer is marked by 1.\n",
        "\n",
        "**Bag of Word(Bow):**\n",
        "A bag of words only describes the occurrence of words within a document or not. It just keeps track of word counts and ignores the grammatical details and the word order.\n",
        "\n",
        "**Bag of n-grams:**\n",
        "  In Bag of Words, there is no consideration of the phrases or word order. Bag of n-gram tries to solve this problem by breaking text into chunks of n continuous words.\n",
        "\n",
        "**TF-IDF (Term Frequency – Inverse Document Frequency):**In all the above techniques,  Each word is treated equally. TF-IDF tries to quantify the importance of a given word relative to the other word in the corpus.\n",
        "\n",
        "*TF(t,d) = (Number of occurrences of term t in document d)/(Total number of terms in the document d)*\n",
        "\n",
        "*IDF(t)= log<sub>e</sub> ((Total number of documents in the corpus)/(Number of documents with term t in corpus))*\n",
        "\n",
        "\n",
        " *TF-IDF Score = TF * IDF*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmFhdTW_9sWx"
      },
      "source": [
        "# **1.3.2 Nueral approch**\n",
        "\n",
        "in the neural approach or word embedding, we try to incorporate the contextual meaning of the words.\n",
        "\n",
        "**1. Train our own embedding layer:**\n",
        "There are two ways to train our own word embedding vector :\n",
        "\n",
        "**CBOW (Continuous Bag of Words):**\n",
        "In this case, we predict the center word from the given set of context words i.e previous and afterwords of the center word.\n",
        "\n",
        "    For example :\n",
        "\n",
        "    I am learning Natural Language Processing from GFG.\n",
        "\n",
        "    I am learning Natural _____?_____ Processing from GFG.\n",
        "**SkipGram:**\n",
        "\n",
        "In this case, we predict the context word from the center word.  \n",
        "\n",
        "    For example :\n",
        "\n",
        "    I am learning Natural Language Processing from GFG.\n",
        "\n",
        "    I am __?___ _____?_____ Language ___?___ ____?____ GFG.\n",
        "\n",
        "**2. Pre-Trained Word Embeddings :**\n",
        "These models are trained on a very large corpus.\n",
        "\n",
        "Some of the most popular pre-trained embeddings are as follows :\n",
        "\n",
        "1. **Word2vec by Google**\n",
        "2. **GloVe by Stanford**\n",
        "3. **fasttext by Facebook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "plpCkLLUAWnj",
        "outputId": "e5cd53af-c112-4301-d158-7726bddb959f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Similarity between 'learn' and 'learning' using Word2Vec: 0.637\n",
            "Similarity between 'india' and 'indian' using Word2Vec: 0.697\n",
            "Similarity between 'fame' and 'famous' using Word2Vec: 0.326\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# load the pre-trained Word2Vec model\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# define word pairs to compute similarity for\n",
        "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
        "\n",
        "# compute similarity for each pair of words\n",
        "for pair in word_pairs:\n",
        "    similarity = model.similarity(pair[0], pair[1])\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ct9JRef7A7Q_",
        "outputId": "5abfc56c-ff6a-481d-bd77-92e3d28aa5fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.840B.300d.zip: 2.18GB [06:49, 5.31MB/s]                            \n",
            "100%|█████████▉| 2196016/2196017 [06:07<00:00, 5975.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'learn' and 'learning' using GloVe: 0.768\n",
            "Similarity between 'india' and 'indian' using GloVe: 0.764\n",
            "Similarity between 'fame' and 'famous' using GloVe: 0.507\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext.vocab as vocab\n",
        "\n",
        "# load the pre-trained GloVe model\n",
        "glove = vocab.GloVe(name='840B', dim=300)\n",
        "\n",
        "# define word pairs to compute similarity for\n",
        "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
        "\n",
        "# compute similarity for each pair of words\n",
        "for pair in word_pairs:\n",
        "    vec1, vec2 = glove[pair[0]], glove[pair[1]]\n",
        "    similarity = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fm6uuuleBB2M",
        "outputId": "aa964da1-7c4b-4c10-dc67-a3d83db33d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
            "Similarity between 'learn' and 'learning' using Word2Vec: 0.642\n",
            "Similarity between 'india' and 'indian' using Word2Vec: 0.708\n",
            "Similarity between 'fame' and 'famous' using Word2Vec: 0.519\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# load the pre-trained fastText model\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "# define word pairs to compute similarity for\n",
        "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
        "\n",
        "# compute similarity for each pair of words\n",
        "for pair in word_pairs:\n",
        "\tsimilarity = fasttext_model.similarity(pair[0], pair[1])\n",
        "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "187pl4yycoBAhsZaeXWpc0H7ySRZyFaH4",
      "authorship_tag": "ABX9TyNDman7MeU+Xf6+uHFwjoLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}